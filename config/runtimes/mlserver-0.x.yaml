apiVersion: serving.kserve.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: mlserver-0.x
spec:
  supportedModelTypes:
    - name: sklearn
      version: "0" # v0.23.1
    - name: xgboost
      version: "1" # v1.1.1
    - name: lightgbm
      version: "3" # v3.2.1

  containers:
    - name: mlserver
      image: mlserver-0:replace
      env:
        - name: MLSERVER_MODELS_DIR
          value: "/models/_mlserver_models/"
        - name: MLSERVER_GRPC_PORT
          value: "8001"
        # default value for HTTP port is 8080 which conflicts with MMesh's
        # Litelinks port
        - name: MLSERVER_HTTP_PORT
          value: "8002"
        - name: MLSERVER_LOAD_MODELS_AT_STARTUP
          value: "false"
        # Set a dummy model name via environment so that MLServer doesn't
        # error on a RepositoryIndex call when no models exist
        - name: MLSERVER_MODEL_NAME
          value: dummy-model-fixme
        # Set server addr to localhost to ensure MLServer only listen inside the pod
        - name: MLSERVER_HOST
          value: "127.0.0.1"
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "5"
          memory: 1Gi
