apiVersion: serving.kserve.io/v1beta1
kind: ServingRuntime
metadata:
  name: triton-2.x
  labels:
    name: modelmesh-serving-triton-2.x-SR
  annotations:
    maxLoadingConcurrency: "2"
spec:
  supportedModelTypes:
    - name: tensorflow
      version: "1" # 1.15.4
    - name: tensorflow
      version: "2" # 2.3.1
    - name: tensorrt
      version: "7" # 7.2.1
    - name: pytorch
      version: "1" # 1.8.0a0+17f8c32
    - name: onnx
      version: "1" # 1.5.3

  grpcEndpoint: "port:8085"
  grpcDataEndpoint: "port:8001"

  containers:
    - name: triton
      image: tritonserver-2:replace
      command: [/bin/sh]
      args:
        - -c
        - 'mkdir -p /models/_triton_models;
          chmod 777 /models/_triton_models;
          exec tritonserver
          "--model-repository=/models/_triton_models"
          "--model-control-mode=explicit"
          "--strict-model-config=false"
          "--strict-readiness=false"
          "--allow-http=true"
          "--allow-sagemaker=false"
          '
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "5"
          memory: 1Gi
      livenessProbe:
        # the server is listening only on 127.0.0.1, so an httpGet probe sent
        # from the kublet running on the node cannot connect to the server
        # (not even with the Host header or host field)
        # exec a curl call to have the request originate from localhost in the
        # container
        exec:
          command:
            - curl
            - --fail
            - --silent
            - --show-error
            - --max-time
            - "9"
            - http://localhost:8000/v2/health/live
        initialDelaySeconds: 5
        periodSeconds: 30
        timeoutSeconds: 10
  builtInAdapter:
    serverType: "triton"
    runtimeManagementPort: 8001
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
